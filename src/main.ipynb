{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Torch imports\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom imports\n",
    "import torch\n",
    "import heuristics\n",
    "from dataset_utils import PathPlanningDataset\n",
    "from display import Displayer\n",
    "from astar_refactored import Astar\n",
    "from custom_types import onehottensor2node2d, nodelist2otensor\n",
    "from training_module import NeuralAstarTrainingModule\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dataset = PathPlanningDataset(\n",
    "    \"/workspaces/neural-Astar/datasets/raw/street\",\n",
    "    16,\n",
    "    1,\n",
    "    heuristics.euclidian,\n",
    "    randomize_points=False,\n",
    "    max_astar_iterations=10000,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(initial_dataset, 1, False)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"metrics/h_mean\", save_weights_only=True, mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | astar | NeuralAstar | 15.9 M\n",
      "--------------------------------------\n",
      "15.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.9 M    Total params\n",
      "63.622    Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURING OPTIMIZERS\n",
      "Epoch 0:   0%|          | 0/90 [00:00<?, ?it/s] "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15,  7]])\n",
      "tensor([[4, 0]])\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/neural-Astar/src/main.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d4173746172222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/neural-Astar/src/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m training_module \u001b[39m=\u001b[39m NeuralAstarTrainingModule()\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d4173746172222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/neural-Astar/src/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d4173746172222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/neural-Astar/src/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d4173746172222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/neural-Astar/src/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d4173746172222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/neural-Astar/src/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[checkpoint_callback],\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d4173746172222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/neural-Astar/src/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d4173746172222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f7374616e69736c61772f6e657572616c2d41737461722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/neural-Astar/src/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(training_module, dataloader)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1036\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(batch_idx, closure)\n\u001b[1;32m    189\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    266\u001b[0m     trainer,\n\u001b[1;32m    267\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    268\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    269\u001b[0m     batch_idx,\n\u001b[1;32m    270\u001b[0m     optimizer,\n\u001b[1;32m    271\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1282\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1244\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1245\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1249\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \n\u001b[1;32m   1281\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/optim/rmsprop.py:106\u001b[0m, in \u001b[0;36mRMSprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 106\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    108\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    109\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspaces/neural-Astar/src/training_module.py:33\u001b[0m, in \u001b[0;36mNeuralAstarTrainingModule.training_step\u001b[0;34m(self, train_batch, batch_idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mmetrics/train_loss\u001b[39m\u001b[39m\"\u001b[39m, loss)\n\u001b[0;32m---> 33\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "training_module = NeuralAstarTrainingModule()\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_every_n_steps=1,\n",
    "    default_root_dir=\".\",\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(training_module, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
